---
# This playbook shrinks the Ceph monitors from your cluster
# It can remove any number of monitor(s) from the cluster and ALL THEIR DATA
#
# Use it like this:
# ansible-playbook shrink-mon.yml -e mon_to_kill=ceph-mon01
#     Prompts for confirmation to shrink, defaults to no and
#     doesn't shrink the cluster. yes shrinks the cluster.
#
# ansible-playbook -e ireallymeanit=yes|no shrink-cluster.yml
#     Overrides the prompt using -e option. Can be used in
#     automation scripts to avoid interactive prompt.


- name: gather facts and check the init system

  hosts:
    - "{{ mon_group_name|default('mons') }}"

  become: True
  tasks:
    - debug: msg="gather facts on all Ceph hosts for following reference"


- name: confirm whether user really meant to remove monitor(s) from the ceph cluster

  hosts:
    - localhost

  become: true

  vars_prompt:
    - name: ireallymeanit
      prompt: Are you sure you want to shrink the cluster?
      default: 'no'
      private: no

  vars:
    mon_group_name: mons

  pre_tasks:
    - name: exit playbook, if only one monitor is present in cluster
      fail:
        msg: "You are about to shrink the only monitor present in the cluster.
              If you really want to do that, please use the purge-cluster playbook."
      when:
        - groups[mon_group_name] | length | int == 1

    - name: exit playbook, if user did not mean to shrink cluster
      fail:
        msg: "Exiting shrink-mon playbook, no monitor(s) was/were removed.
           To shrink the cluster, either say 'yes' on the prompt or
           or use `-e ireallymeanit=yes` on the command line when
           invoking the playbook"
      when:
        - ireallymeanit != 'yes'

    - name: exit playbook, if no monitor(s) was/were given
      fail:
        msg: "mon_to_kill must be declared
          Exiting shrink-cluster playbook, no monitor(s) was/were removed.
           On the command line when invoking the playbook, you can use
           -e mon_to_kill=ceph-mon01 argument. You can only remove a single monitor each time the playbook runs."
      when:
        - mon_to_kill is not defined

  roles:
    - ceph-defaults

  post_tasks:
    - name: pick a monitor different than the one we want to remove
      set_fact: mon_host={{ item }}
      with_items: "{{ groups[mon_group_name] }}"
      when:
        - item != mon_to_kill

    - name: exit playbook, if can not connect to the cluster
      command: timeout 5 ceph --cluster {{ cluster }} health
      register: ceph_health
      until: ceph_health.stdout.find("HEALTH") > -1
      delegate_to: "{{ mon_host }}"
      retries: 5
      delay: 2

    - name: verify given monitor(s) is/are reachable
      command: ping -c 1 {{ mon_to_kill }}
      register: mon_reachable
      delegate_to: "{{ mon_host }}"
      failed_when: false

    - fail:
        msg: "One or more monitors is/are not reachable, please check your /etc/hosts or your DNS"
      when:
        - mon_reachable.rc != 0

    - name: stop monitor service(s)
      service:
        name: ceph-mon@{{ mon_to_kill }}
        state: stopped
        enabled: no
      delegate_to: "{{ mon_to_kill }}"
      failed_when: false

    - name: purge monitor store
      file:
        path: /var/lib/ceph/mon/{{ cluster }}-{{ mon_to_kill }}
        state: absent
      delegate_to: "{{ mon_to_kill }}"

    - name: remove monitor from the quorum
      command: ceph --cluster {{ cluster }} mon remove {{ mon_to_kill }}
      failed_when: false
      delegate_to: "{{ mon_host }}"

    # NOTE (leseb): sorry for the 'sleep' command
    # but it will take a couple of seconds for other monitors
    # to notice that one member has left.
    # 'sleep 5' is not that bad and should be sufficient
    - name: verify the monitor is out of the cluster
      shell: "sleep 5 && timeout 5 ceph --cluster {{ cluster }} -s | grep monmap | sed 's/.*quorum//' | grep -Esq {{ mon_to_kill }}"
      failed_when: false
      register: ceph_health_mon
      delegate_to: "{{ mon_host }}"

    - name: please remove the monitor from your ceph configuration file
      debug:
          msg: "The monitor(s) has/have been successfully removed from the cluster.
          Please remove the monitor(s) entry(ies) from the rest of your ceph configuration files, cluster wide."
      run_once: true
      when:
        - ceph_health_mon.rc != 0

    - name: fail if monitor(s) is/are still part of the cluster
      fail:
          msg: "Monitor(s) appear(s) to still be part of the cluster, please check what happened."
      run_once: true
      when:
        - ceph_health_mon.rc != 0

    - name: show ceph health
      command: ceph --cluster {{ cluster }} -s
      delegate_to: "{{ mon_host }}"
